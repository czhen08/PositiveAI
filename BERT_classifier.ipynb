{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_FXneEfpdMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch_pretrained_bert pytorch-nlp\n",
        "!apt install unixodbc-dev\n",
        "!pip install pyodbc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9aO80TLiGGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%sh\n",
        "curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n",
        "curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list > /etc/apt/sources.list.d/mssql-release.list\n",
        "sudo apt-get update\n",
        "sudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql17"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EnVIV6Vt8f4d",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import torch\n",
        "import pyodbc\n",
        "import csv\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "from torch import nn\n",
        "from torchnlp.datasets import imdb_dataset\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q15FCqWt05j2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rn.seed(2020)\n",
        "np.random.seed(2020)\n",
        "torch.manual_seed(2020)\n",
        "torch.cuda.manual_seed(2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgkbhHcB17GY",
        "colab_type": "text"
      },
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRLw3qmegTrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the database configurations and functions to query dataset from the database\n",
        "\n",
        "server = 'positive-ai-server.database.windows.net'\n",
        "database = 'positive-ai-db'\n",
        "username = 'positive-ai-admin'\n",
        "password = 'Zcdukic001'\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\n",
        "\n",
        "def query_training_set():\n",
        "    cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password)\n",
        "    cursor = cnxn.cursor()\n",
        "    cursor.execute(\"select Text, Sentiment from training_dataset_new\")\n",
        "    texts = []\n",
        "    labels = []\n",
        "    rows = cursor.fetchall()\n",
        "    for row in rows:\n",
        "        texts.append(row.Text)\n",
        "        labels.append(row.Sentiment)\n",
        "    return texts,labels\n",
        "\n",
        "def query_testing_set():\n",
        "    cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password)\n",
        "    cursor = cnxn.cursor()\n",
        "    cursor.execute(\"select Text, Sentiment from testing_dataset\")\n",
        "    texts = []\n",
        "    labels = []\n",
        "    rows = cursor.fetchall()\n",
        "    for row in rows:\n",
        "        texts.append(row.Text)\n",
        "        labels.append(row.Sentiment)\n",
        "    return texts,labels\n",
        "\n",
        "def query_dataset(db):\n",
        "    cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password)\n",
        "    cursor = cnxn.cursor()\n",
        "    cursor.execute(\"select text, sentiment from \"+db)\n",
        "    texts = []\n",
        "    labels = []\n",
        "    rows = cursor.fetchall()\n",
        "    for row in rows:\n",
        "        texts.append(row.text)\n",
        "        labels.append(row.sentiment)\n",
        "    return texts,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AdjHEb7dJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare the trainging set, validation set, and testing set\n",
        "\n",
        "good_news_texts, good_news_labels = query_dataset(\"Good_News\") \n",
        "pos_web_texts, pos_web_labels = query_dataset(\"Pos_Web\") \n",
        "bad_news_texts, bad_news_labels = query_dataset(\"Bad_News\") \n",
        "neg_news_texts, neg_news_labels = query_dataset(\"Neg_News\")\n",
        "new_texts, new_labels = query_training_set()\n",
        "\n",
        "train_texts = good_news_texts + pos_web_texts[:500] + bad_news_texts[:9580] + neg_news_texts + new_texts\n",
        "train_labels = good_news_labels + pos_web_labels[:500] + bad_news_labels[:9580] + neg_news_labels + new_labels\n",
        "\n",
        "val_texts = pos_web_texts[500:1000] + bad_news_texts[9580:10080]\n",
        "val_labels = pos_web_labels[500:1000] + bad_news_texts[9580:10080]\n",
        "\n",
        "# train_texts = good_news_texts + new_texts + pos_web_texts[:1000] + bad_news_texts + neg_news_texts\n",
        "# train_labels = good_news_labels + new_labels + pos_web_labels[:1000] + bad_news_labels + neg_news_labels\n",
        "# val_texts, val_labels = query_testing_set()\n",
        "test_texts, test_labels = query_testing_set()\n",
        "\n",
        "train_data = train_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrqfluQgMgmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle the training set\n",
        "data = list(zip(train_texts, train_labels))\n",
        "rn.shuffle(data)\n",
        "train_texts, train_labels = zip(*data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty24UrRjqIsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k9rcOzQr5Zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenization\n",
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\n",
        "val_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], val_texts))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))                    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ca7KKnhuT5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert tokens to ids, pad with zeros if needed\n",
        "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "val_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, val_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F7POtHuIOV-6",
        "colab": {}
      },
      "source": [
        "# Prepare the labels\n",
        "train_y = np.array(train_labels) == 'pos'\n",
        "val_y = np.array(val_labels) == 'pos'\n",
        "test_y = np.array(test_labels) == 'pos'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-xXMEqXOWTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention mask\n",
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "val_masks = [[float(i > 0) for i in ii] for ii in val_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_hEhebQ3YqI",
        "colab_type": "text"
      },
      "source": [
        "# Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E234ByBa3Qtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create BertClassifier\n",
        "\n",
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens, masks=None):\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED9SE1Ka8W9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify the GPU as the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sf9n8zouENRi",
        "colab": {}
      },
      "source": [
        "# Instantiate Bert Classifier\n",
        "bert_clf = BertBinaryClassifier()\n",
        "bert_clf = bert_clf.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9LPIYcn99r8",
        "colab_type": "text"
      },
      "source": [
        "# Traing and Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUkXhM1k_TAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set batch size and epochs\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGwV0yqg_o2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert data to torch.tensor\n",
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
        "\n",
        "val_tokens_tensor = torch.tensor(val_tokens_ids)\n",
        "val_y_tensor = torch.tensor(val_y.reshape(-1, 1)).float()\n",
        "\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "val_masks_tensor = torch.tensor(val_masks)\n",
        "test_masks_tensor = torch.tensor(test_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yl2JpCe9YAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the DataLoader for training, validation and testing set\n",
        "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "val_dataset = TensorDataset(val_tokens_tensor, val_masks_tensor, val_y_tensor)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF_QD0naS8EQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the optimizer\n",
        "param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n",
        "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "optimizer = Adam(bert_clf.parameters(), lr=3e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3_ZsCtJaa0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create evaluate and test functions\n",
        "def evaluate(bert_clf, val_dataloader):\n",
        "    \"\"\"\n",
        "    After the completion of each training epoch, measure the model's performance on validation set.\n",
        "    \"\"\"\n",
        "    bert_clf.eval()\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            logits = bert_clf(token_ids, masks)\n",
        "\n",
        "        loss_func = nn.BCELoss()\n",
        "        loss = loss_func(logits, labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "        accuracy = (preds == labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "def test(bert_clf, test_dataloader):\n",
        "    \"\"\"\n",
        "    After the completion of whole training process, measure the model's performance on testing set.\n",
        "    \"\"\"\n",
        "    bert_clf.eval()\n",
        "    bert_predicted = []\n",
        "    all_logits = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for step_num, batch_data in enumerate(test_dataloader):\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        loss = loss_func(logits, labels)\n",
        "        numpy_logits = logits.cpu().detach().numpy()  \n",
        "        \n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "        all_logits += list(numpy_logits[:, 0])\n",
        "\n",
        "    print(classification_report(test_y, bert_predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqh8tCl4AFjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The training process\n",
        "for epoch_num in range(EPOCHS):\n",
        "    bert_clf.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        \n",
        "        loss_func = nn.BCELoss()\n",
        "        batch_loss = loss_func(logits, labels)\n",
        "        train_loss += batch_loss.item()\n",
        "        \n",
        "        bert_clf.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        \n",
        "\n",
        "        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (step_num % 2000 == 0 and step_num != 0) or (step_num == len(train_dataloader) - 1):\n",
        "          # print('Epoch: ', epoch_num + 1)\n",
        "          print(\"Epoch:{0} {1}/{2} loss: {3} \".format(epoch_num + 1, step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
        "    \n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    val_loss, val_accuracy = evaluate(bert_clf, val_dataloader)\n",
        "    print(\"Train loss: \"+str(avg_train_loss)+\" Val loss :\"+str(val_loss)+\" Val acc: \"+str(val_accuracy))\n",
        "    test(bert_clf, test_dataloader)\n",
        "    print(\"-\"*70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3QRRRe6TXuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model\n",
        "torch.save(bert_clf.state_dict(), 'bert_model.bin')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}